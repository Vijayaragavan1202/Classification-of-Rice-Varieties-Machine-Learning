{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e1b39f-da91-4ee2-8491-474b2612aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85c1c3f-943e-40b6-9188-c834e2d46264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rice data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2c9dab-be93-458e-8232-4dc95419c264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>ALLdaub4L</th>\n",
       "      <th>ALLdaub4a</th>\n",
       "      <th>ALLdaub4b</th>\n",
       "      <th>ALLdaub4Y</th>\n",
       "      <th>ALLdaub4Cb</th>\n",
       "      <th>ALLdaub4Cr</th>\n",
       "      <th>ALLdaub4XX</th>\n",
       "      <th>ALLdaub4YY</th>\n",
       "      <th>ALLdaub4ZZ</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7805</td>\n",
       "      <td>437.915</td>\n",
       "      <td>209.8215</td>\n",
       "      <td>48.0221</td>\n",
       "      <td>0.9735</td>\n",
       "      <td>99.6877</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>7985</td>\n",
       "      <td>0.3547</td>\n",
       "      <td>4.3693</td>\n",
       "      <td>...</td>\n",
       "      <td>113.9924</td>\n",
       "      <td>65.0610</td>\n",
       "      <td>59.5989</td>\n",
       "      <td>104.8552</td>\n",
       "      <td>67.8779</td>\n",
       "      <td>63.0828</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>0.3793</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>Basmati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7503</td>\n",
       "      <td>340.757</td>\n",
       "      <td>138.3361</td>\n",
       "      <td>69.8417</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>97.7400</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>7767</td>\n",
       "      <td>0.6637</td>\n",
       "      <td>1.9807</td>\n",
       "      <td>...</td>\n",
       "      <td>105.7055</td>\n",
       "      <td>64.3685</td>\n",
       "      <td>62.2084</td>\n",
       "      <td>96.8375</td>\n",
       "      <td>65.5371</td>\n",
       "      <td>63.5832</td>\n",
       "      <td>0.3014</td>\n",
       "      <td>0.3144</td>\n",
       "      <td>0.3641</td>\n",
       "      <td>Arborio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5124</td>\n",
       "      <td>314.617</td>\n",
       "      <td>141.9803</td>\n",
       "      <td>46.5784</td>\n",
       "      <td>0.9447</td>\n",
       "      <td>80.7718</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>5271</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>3.0482</td>\n",
       "      <td>...</td>\n",
       "      <td>109.7155</td>\n",
       "      <td>62.6423</td>\n",
       "      <td>58.7439</td>\n",
       "      <td>100.2352</td>\n",
       "      <td>68.9753</td>\n",
       "      <td>59.8342</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>0.3445</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>Jasmine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7990</td>\n",
       "      <td>437.085</td>\n",
       "      <td>201.4386</td>\n",
       "      <td>51.2245</td>\n",
       "      <td>0.9671</td>\n",
       "      <td>100.8622</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>8272</td>\n",
       "      <td>0.6274</td>\n",
       "      <td>3.9325</td>\n",
       "      <td>...</td>\n",
       "      <td>116.5405</td>\n",
       "      <td>64.9069</td>\n",
       "      <td>60.2562</td>\n",
       "      <td>107.2560</td>\n",
       "      <td>67.3298</td>\n",
       "      <td>63.2237</td>\n",
       "      <td>0.3880</td>\n",
       "      <td>0.4020</td>\n",
       "      <td>0.4904</td>\n",
       "      <td>Basmati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7433</td>\n",
       "      <td>342.893</td>\n",
       "      <td>140.3350</td>\n",
       "      <td>68.3927</td>\n",
       "      <td>0.8732</td>\n",
       "      <td>97.2830</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>7561</td>\n",
       "      <td>0.6006</td>\n",
       "      <td>2.0519</td>\n",
       "      <td>...</td>\n",
       "      <td>107.7502</td>\n",
       "      <td>64.7071</td>\n",
       "      <td>61.3549</td>\n",
       "      <td>98.8704</td>\n",
       "      <td>66.2048</td>\n",
       "      <td>63.5378</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.3928</td>\n",
       "      <td>Arborio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  SOLIDITY  \\\n",
       "0  7805    437.915    209.8215     48.0221        0.9735   99.6877    0.9775   \n",
       "1  7503    340.757    138.3361     69.8417        0.8632   97.7400    0.9660   \n",
       "2  5124    314.617    141.9803     46.5784        0.9447   80.7718    0.9721   \n",
       "3  7990    437.085    201.4386     51.2245        0.9671  100.8622    0.9659   \n",
       "4  7433    342.893    140.3350     68.3927        0.8732   97.2830    0.9831   \n",
       "\n",
       "   CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  ALLdaub4L  ALLdaub4a  ALLdaub4b  \\\n",
       "0         7985  0.3547        4.3693  ...   113.9924    65.0610    59.5989   \n",
       "1         7767  0.6637        1.9807  ...   105.7055    64.3685    62.2084   \n",
       "2         5271  0.4760        3.0482  ...   109.7155    62.6423    58.7439   \n",
       "3         8272  0.6274        3.9325  ...   116.5405    64.9069    60.2562   \n",
       "4         7561  0.6006        2.0519  ...   107.7502    64.7071    61.3549   \n",
       "\n",
       "   ALLdaub4Y  ALLdaub4Cb  ALLdaub4Cr  ALLdaub4XX  ALLdaub4YY  ALLdaub4ZZ  \\\n",
       "0   104.8552     67.8779     63.0828      0.3673      0.3793      0.4733   \n",
       "1    96.8375     65.5371     63.5832      0.3014      0.3144      0.3641   \n",
       "2   100.2352     68.9753     59.8342      0.3233      0.3445      0.4448   \n",
       "3   107.2560     67.3298     63.2237      0.3880      0.4020      0.4904   \n",
       "4    98.8704     66.2048     63.5378      0.3184      0.3303      0.3928   \n",
       "\n",
       "     CLASS  \n",
       "0  Basmati  \n",
       "1  Arborio  \n",
       "2  Jasmine  \n",
       "3  Basmati  \n",
       "4  Arborio  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9ab28cd-6f11-4726-b695-62b3f14887ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[:,16:106]\n",
    "df1[\"CLASS\"] = df[\"CLASS\"]\n",
    "df = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3aa2db-40af-42b0-9d5c-9f5f15172601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 91)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37893170-ddd7-46fe-a74d-967f3b3bb785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "Basmati      15000\n",
       "Arborio      15000\n",
       "Jasmine      15000\n",
       "Ipsala       15000\n",
       "Karacadag    15000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"CLASS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad778277-8b3c-4f69-8b8e-287b8c9cfb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75000 entries, 0 to 74999\n",
      "Data columns (total 91 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   meanRR      75000 non-null  float64\n",
      " 1   meanRG      75000 non-null  float64\n",
      " 2   meanRB      75000 non-null  float64\n",
      " 3   StdDevRR    75000 non-null  float64\n",
      " 4   StdDevRG    75000 non-null  float64\n",
      " 5   StdDevRB    75000 non-null  float64\n",
      " 6   skewRR      75000 non-null  float64\n",
      " 7   skewRG      75000 non-null  float64\n",
      " 8   skewRB      75000 non-null  float64\n",
      " 9   kurtosisRR  75000 non-null  float64\n",
      " 10  kurtosisRG  75000 non-null  float64\n",
      " 11  kurtosisRB  75000 non-null  float64\n",
      " 12  entropyRR   75000 non-null  int64  \n",
      " 13  entropyRG   75000 non-null  int64  \n",
      " 14  entropyRB   75000 non-null  int64  \n",
      " 15  meanH       75000 non-null  float64\n",
      " 16  meanS       75000 non-null  float64\n",
      " 17  meanV       75000 non-null  float64\n",
      " 18  StdDevH     75000 non-null  float64\n",
      " 19  StdDevS     75000 non-null  float64\n",
      " 20  StdDevV     75000 non-null  float64\n",
      " 21  skewH       75000 non-null  float64\n",
      " 22  skewS       75000 non-null  float64\n",
      " 23  skewV       75000 non-null  float64\n",
      " 24  kurtosisH   75000 non-null  float64\n",
      " 25  kurtosisS   75000 non-null  float64\n",
      " 26  kurtosisV   75000 non-null  float64\n",
      " 27  entropyH    75000 non-null  float64\n",
      " 28  entropyS    75000 non-null  float64\n",
      " 29  entropyV    75000 non-null  float64\n",
      " 30  meanL       75000 non-null  float64\n",
      " 31  meanA       75000 non-null  float64\n",
      " 32  meanB       75000 non-null  float64\n",
      " 33  StdDevL     75000 non-null  float64\n",
      " 34  StdDevA     75000 non-null  float64\n",
      " 35  StdDevB     75000 non-null  float64\n",
      " 36  skewL       75000 non-null  float64\n",
      " 37  skewA       75000 non-null  float64\n",
      " 38  skewB       74994 non-null  float64\n",
      " 39  kurtosisL   75000 non-null  float64\n",
      " 40  kurtosisA   75000 non-null  float64\n",
      " 41  kurtosisB   74994 non-null  float64\n",
      " 42  entropyL    75000 non-null  int64  \n",
      " 43  entropyA    75000 non-null  int64  \n",
      " 44  entropyB    75000 non-null  int64  \n",
      " 45  meanY       75000 non-null  float64\n",
      " 46  meanCb      75000 non-null  float64\n",
      " 47  meanCr      75000 non-null  float64\n",
      " 48  StdDevY     75000 non-null  float64\n",
      " 49  StdDevCb    75000 non-null  float64\n",
      " 50  StdDevCr    75000 non-null  float64\n",
      " 51  skewY       75000 non-null  float64\n",
      " 52  skewCb      74997 non-null  float64\n",
      " 53  skewCr      74998 non-null  float64\n",
      " 54  kurtosisY   75000 non-null  float64\n",
      " 55  kurtosisCb  74997 non-null  float64\n",
      " 56  kurtosisCr  74998 non-null  float64\n",
      " 57  entropyY    75000 non-null  int64  \n",
      " 58  entropyCb   75000 non-null  int64  \n",
      " 59  entropyCr   75000 non-null  int64  \n",
      " 60  meanXX      75000 non-null  float64\n",
      " 61  meanYY      75000 non-null  float64\n",
      " 62  meanZZ      75000 non-null  float64\n",
      " 63  StdDevXX    75000 non-null  float64\n",
      " 64  StdDevYY    75000 non-null  float64\n",
      " 65  StdDevZZ    75000 non-null  float64\n",
      " 66  skewXX      75000 non-null  float64\n",
      " 67  skewYY      75000 non-null  float64\n",
      " 68  skewZZ      75000 non-null  float64\n",
      " 69  kurtosisXX  75000 non-null  float64\n",
      " 70  kurtosisYY  75000 non-null  float64\n",
      " 71  kurtosisZZ  75000 non-null  float64\n",
      " 72  entropyXX   75000 non-null  float64\n",
      " 73  entropyYY   75000 non-null  float64\n",
      " 74  entropyZZ   75000 non-null  float64\n",
      " 75  ALLdaub4RR  75000 non-null  float64\n",
      " 76  ALLdaub4RG  75000 non-null  float64\n",
      " 77  ALLdaub4RB  75000 non-null  float64\n",
      " 78  ALLdaub4H   75000 non-null  float64\n",
      " 79  ALLdaub4S   75000 non-null  float64\n",
      " 80  ALLdaub4V   75000 non-null  float64\n",
      " 81  ALLdaub4L   75000 non-null  float64\n",
      " 82  ALLdaub4a   75000 non-null  float64\n",
      " 83  ALLdaub4b   75000 non-null  float64\n",
      " 84  ALLdaub4Y   75000 non-null  float64\n",
      " 85  ALLdaub4Cb  75000 non-null  float64\n",
      " 86  ALLdaub4Cr  75000 non-null  float64\n",
      " 87  ALLdaub4XX  75000 non-null  float64\n",
      " 88  ALLdaub4YY  75000 non-null  float64\n",
      " 89  ALLdaub4ZZ  75000 non-null  float64\n",
      " 90  CLASS       75000 non-null  object \n",
      "dtypes: float64(81), int64(9), object(1)\n",
      "memory usage: 52.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bafeae6f-957e-4f69-be65-5729bac65755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanRR</th>\n",
       "      <th>meanRG</th>\n",
       "      <th>meanRB</th>\n",
       "      <th>StdDevRR</th>\n",
       "      <th>StdDevRG</th>\n",
       "      <th>StdDevRB</th>\n",
       "      <th>skewRR</th>\n",
       "      <th>skewRG</th>\n",
       "      <th>skewRB</th>\n",
       "      <th>kurtosisRR</th>\n",
       "      <th>...</th>\n",
       "      <th>ALLdaub4V</th>\n",
       "      <th>ALLdaub4L</th>\n",
       "      <th>ALLdaub4a</th>\n",
       "      <th>ALLdaub4b</th>\n",
       "      <th>ALLdaub4Y</th>\n",
       "      <th>ALLdaub4Cb</th>\n",
       "      <th>ALLdaub4Cr</th>\n",
       "      <th>ALLdaub4XX</th>\n",
       "      <th>ALLdaub4YY</th>\n",
       "      <th>ALLdaub4ZZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>216.398005</td>\n",
       "      <td>218.205782</td>\n",
       "      <td>227.918353</td>\n",
       "      <td>15.342766</td>\n",
       "      <td>15.449838</td>\n",
       "      <td>15.477779</td>\n",
       "      <td>-1.778549</td>\n",
       "      <td>-1.938456</td>\n",
       "      <td>-2.360081</td>\n",
       "      <td>11.955533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448960</td>\n",
       "      <td>111.088252</td>\n",
       "      <td>64.379443</td>\n",
       "      <td>61.461457</td>\n",
       "      <td>101.925425</td>\n",
       "      <td>66.240541</td>\n",
       "      <td>63.202088</td>\n",
       "      <td>0.341944</td>\n",
       "      <td>0.357058</td>\n",
       "      <td>0.421176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.308330</td>\n",
       "      <td>13.646445</td>\n",
       "      <td>10.682523</td>\n",
       "      <td>3.454178</td>\n",
       "      <td>3.562578</td>\n",
       "      <td>3.468618</td>\n",
       "      <td>0.948735</td>\n",
       "      <td>1.111904</td>\n",
       "      <td>0.950987</td>\n",
       "      <td>7.479528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>5.904854</td>\n",
       "      <td>1.175616</td>\n",
       "      <td>2.435635</td>\n",
       "      <td>5.436861</td>\n",
       "      <td>2.159109</td>\n",
       "      <td>1.174976</td>\n",
       "      <td>0.041921</td>\n",
       "      <td>0.047139</td>\n",
       "      <td>0.043137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>153.800000</td>\n",
       "      <td>157.249900</td>\n",
       "      <td>160.158400</td>\n",
       "      <td>6.817100</td>\n",
       "      <td>6.411700</td>\n",
       "      <td>6.417500</td>\n",
       "      <td>-6.938800</td>\n",
       "      <td>-7.911800</td>\n",
       "      <td>-6.938200</td>\n",
       "      <td>1.841300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>82.300600</td>\n",
       "      <td>59.137900</td>\n",
       "      <td>53.653800</td>\n",
       "      <td>75.191800</td>\n",
       "      <td>58.323800</td>\n",
       "      <td>57.363400</td>\n",
       "      <td>0.159700</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>206.605125</td>\n",
       "      <td>207.848625</td>\n",
       "      <td>220.927750</td>\n",
       "      <td>12.579400</td>\n",
       "      <td>12.741500</td>\n",
       "      <td>13.050675</td>\n",
       "      <td>-2.360500</td>\n",
       "      <td>-2.522300</td>\n",
       "      <td>-3.002300</td>\n",
       "      <td>6.481900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>106.632900</td>\n",
       "      <td>63.883800</td>\n",
       "      <td>59.465575</td>\n",
       "      <td>97.834400</td>\n",
       "      <td>64.842000</td>\n",
       "      <td>63.052800</td>\n",
       "      <td>0.309900</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>215.118800</td>\n",
       "      <td>217.137550</td>\n",
       "      <td>228.801250</td>\n",
       "      <td>15.542900</td>\n",
       "      <td>15.686150</td>\n",
       "      <td>15.539300</td>\n",
       "      <td>-1.608600</td>\n",
       "      <td>-1.683350</td>\n",
       "      <td>-2.321100</td>\n",
       "      <td>9.727700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451600</td>\n",
       "      <td>110.770700</td>\n",
       "      <td>64.419350</td>\n",
       "      <td>61.424400</td>\n",
       "      <td>101.683700</td>\n",
       "      <td>66.291600</td>\n",
       "      <td>63.522050</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>0.353300</td>\n",
       "      <td>0.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>225.016125</td>\n",
       "      <td>227.339300</td>\n",
       "      <td>236.171600</td>\n",
       "      <td>17.881000</td>\n",
       "      <td>18.032225</td>\n",
       "      <td>17.891800</td>\n",
       "      <td>-1.095000</td>\n",
       "      <td>-1.136700</td>\n",
       "      <td>-1.609700</td>\n",
       "      <td>15.068550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466100</td>\n",
       "      <td>115.065075</td>\n",
       "      <td>65.174200</td>\n",
       "      <td>63.076825</td>\n",
       "      <td>105.592450</td>\n",
       "      <td>68.011800</td>\n",
       "      <td>63.734000</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>0.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>252.183700</td>\n",
       "      <td>252.323100</td>\n",
       "      <td>252.108500</td>\n",
       "      <td>29.967400</td>\n",
       "      <td>30.765400</td>\n",
       "      <td>30.858000</td>\n",
       "      <td>0.917900</td>\n",
       "      <td>0.771900</td>\n",
       "      <td>1.162400</td>\n",
       "      <td>75.201600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495100</td>\n",
       "      <td>126.265100</td>\n",
       "      <td>67.459000</td>\n",
       "      <td>70.284000</td>\n",
       "      <td>116.287300</td>\n",
       "      <td>73.424700</td>\n",
       "      <td>66.539100</td>\n",
       "      <td>0.463900</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>0.530200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             meanRR        meanRG        meanRB      StdDevRR      StdDevRG  \\\n",
       "count  75000.000000  75000.000000  75000.000000  75000.000000  75000.000000   \n",
       "mean     216.398005    218.205782    227.918353     15.342766     15.449838   \n",
       "std       13.308330     13.646445     10.682523      3.454178      3.562578   \n",
       "min      153.800000    157.249900    160.158400      6.817100      6.411700   \n",
       "25%      206.605125    207.848625    220.927750     12.579400     12.741500   \n",
       "50%      215.118800    217.137550    228.801250     15.542900     15.686150   \n",
       "75%      225.016125    227.339300    236.171600     17.881000     18.032225   \n",
       "max      252.183700    252.323100    252.108500     29.967400     30.765400   \n",
       "\n",
       "           StdDevRB        skewRR        skewRG        skewRB    kurtosisRR  \\\n",
       "count  75000.000000  75000.000000  75000.000000  75000.000000  75000.000000   \n",
       "mean      15.477779     -1.778549     -1.938456     -2.360081     11.955533   \n",
       "std        3.468618      0.948735      1.111904      0.950987      7.479528   \n",
       "min        6.417500     -6.938800     -7.911800     -6.938200      1.841300   \n",
       "25%       13.050675     -2.360500     -2.522300     -3.002300      6.481900   \n",
       "50%       15.539300     -1.608600     -1.683350     -2.321100      9.727700   \n",
       "75%       17.891800     -1.095000     -1.136700     -1.609700     15.068550   \n",
       "max       30.858000      0.917900      0.771900      1.162400     75.201600   \n",
       "\n",
       "       ...     ALLdaub4V     ALLdaub4L     ALLdaub4a     ALLdaub4b  \\\n",
       "count  ...  75000.000000  75000.000000  75000.000000  75000.000000   \n",
       "mean   ...      0.448960    111.088252     64.379443     61.461457   \n",
       "std    ...      0.021736      5.904854      1.175616      2.435635   \n",
       "min    ...      0.313900     82.300600     59.137900     53.653800   \n",
       "25%    ...      0.434200    106.632900     63.883800     59.465575   \n",
       "50%    ...      0.451600    110.770700     64.419350     61.424400   \n",
       "75%    ...      0.466100    115.065075     65.174200     63.076825   \n",
       "max    ...      0.495100    126.265100     67.459000     70.284000   \n",
       "\n",
       "          ALLdaub4Y    ALLdaub4Cb    ALLdaub4Cr    ALLdaub4XX    ALLdaub4YY  \\\n",
       "count  75000.000000  75000.000000  75000.000000  75000.000000  75000.000000   \n",
       "mean     101.925425     66.240541     63.202088      0.341944      0.357058   \n",
       "std        5.436861      2.159109      1.174976      0.041921      0.047139   \n",
       "min       75.191800     58.323800     57.363400      0.159700      0.169000   \n",
       "25%       97.834400     64.842000     63.052800      0.309900      0.320900   \n",
       "50%      101.683700     66.291600     63.522050      0.340100      0.353300   \n",
       "75%      105.592450     68.011800     63.734000      0.370300      0.387900   \n",
       "max      116.287300     73.424700     66.539100      0.463900      0.488600   \n",
       "\n",
       "         ALLdaub4ZZ  \n",
       "count  75000.000000  \n",
       "mean       0.421176  \n",
       "std        0.043137  \n",
       "min        0.191800  \n",
       "25%        0.391200  \n",
       "50%        0.424200  \n",
       "75%        0.454700  \n",
       "max        0.530200  \n",
       "\n",
       "[8 rows x 90 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44be88a6-6d31-454f-bfe8-91a0b277bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78f82cd5-e094-4560-a546-67575be0007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(df.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "637293d9-a9e2-4a0f-8b02-26c116de4152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanRR</th>\n",
       "      <th>meanRG</th>\n",
       "      <th>meanRB</th>\n",
       "      <th>StdDevRR</th>\n",
       "      <th>StdDevRG</th>\n",
       "      <th>StdDevRB</th>\n",
       "      <th>skewRR</th>\n",
       "      <th>skewRG</th>\n",
       "      <th>skewRB</th>\n",
       "      <th>kurtosisRR</th>\n",
       "      <th>...</th>\n",
       "      <th>ALLdaub4L</th>\n",
       "      <th>ALLdaub4a</th>\n",
       "      <th>ALLdaub4b</th>\n",
       "      <th>ALLdaub4Y</th>\n",
       "      <th>ALLdaub4Cb</th>\n",
       "      <th>ALLdaub4Cr</th>\n",
       "      <th>ALLdaub4XX</th>\n",
       "      <th>ALLdaub4YY</th>\n",
       "      <th>ALLdaub4ZZ</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>222.9805</td>\n",
       "      <td>223.9872</td>\n",
       "      <td>241.4758</td>\n",
       "      <td>16.2950</td>\n",
       "      <td>16.4354</td>\n",
       "      <td>13.6272</td>\n",
       "      <td>-0.7986</td>\n",
       "      <td>-0.8407</td>\n",
       "      <td>-3.7377</td>\n",
       "      <td>7.1706</td>\n",
       "      <td>...</td>\n",
       "      <td>113.9924</td>\n",
       "      <td>65.0610</td>\n",
       "      <td>59.5989</td>\n",
       "      <td>104.8552</td>\n",
       "      <td>67.8779</td>\n",
       "      <td>63.0828</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>0.3793</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>Basmati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206.0380</td>\n",
       "      <td>206.2412</td>\n",
       "      <td>213.3809</td>\n",
       "      <td>18.3863</td>\n",
       "      <td>18.5343</td>\n",
       "      <td>18.9969</td>\n",
       "      <td>-0.7536</td>\n",
       "      <td>-0.7372</td>\n",
       "      <td>-0.8217</td>\n",
       "      <td>4.4926</td>\n",
       "      <td>...</td>\n",
       "      <td>105.7055</td>\n",
       "      <td>64.3685</td>\n",
       "      <td>62.2084</td>\n",
       "      <td>96.8375</td>\n",
       "      <td>65.5371</td>\n",
       "      <td>63.5832</td>\n",
       "      <td>0.3014</td>\n",
       "      <td>0.3144</td>\n",
       "      <td>0.3641</td>\n",
       "      <td>Arborio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201.8228</td>\n",
       "      <td>217.6475</td>\n",
       "      <td>235.0057</td>\n",
       "      <td>13.7392</td>\n",
       "      <td>15.3239</td>\n",
       "      <td>16.0249</td>\n",
       "      <td>-2.2606</td>\n",
       "      <td>-2.6764</td>\n",
       "      <td>-2.8690</td>\n",
       "      <td>12.4329</td>\n",
       "      <td>...</td>\n",
       "      <td>109.7155</td>\n",
       "      <td>62.6423</td>\n",
       "      <td>58.7439</td>\n",
       "      <td>100.2352</td>\n",
       "      <td>68.9753</td>\n",
       "      <td>59.8342</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>0.3445</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>Jasmine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>228.8978</td>\n",
       "      <td>229.7151</td>\n",
       "      <td>244.6294</td>\n",
       "      <td>18.3915</td>\n",
       "      <td>18.9141</td>\n",
       "      <td>16.7398</td>\n",
       "      <td>-1.5281</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-3.5705</td>\n",
       "      <td>8.1541</td>\n",
       "      <td>...</td>\n",
       "      <td>116.5405</td>\n",
       "      <td>64.9069</td>\n",
       "      <td>60.2562</td>\n",
       "      <td>107.2560</td>\n",
       "      <td>67.3298</td>\n",
       "      <td>63.2237</td>\n",
       "      <td>0.3880</td>\n",
       "      <td>0.4020</td>\n",
       "      <td>0.4904</td>\n",
       "      <td>Basmati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>210.4471</td>\n",
       "      <td>210.2988</td>\n",
       "      <td>220.8827</td>\n",
       "      <td>19.6370</td>\n",
       "      <td>19.6635</td>\n",
       "      <td>20.0206</td>\n",
       "      <td>-1.0165</td>\n",
       "      <td>-1.0176</td>\n",
       "      <td>-1.1514</td>\n",
       "      <td>4.6467</td>\n",
       "      <td>...</td>\n",
       "      <td>107.7502</td>\n",
       "      <td>64.7071</td>\n",
       "      <td>61.3549</td>\n",
       "      <td>98.8704</td>\n",
       "      <td>66.2048</td>\n",
       "      <td>63.5378</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.3928</td>\n",
       "      <td>Arborio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>204.4210</td>\n",
       "      <td>201.7319</td>\n",
       "      <td>208.9247</td>\n",
       "      <td>13.8755</td>\n",
       "      <td>13.8819</td>\n",
       "      <td>14.0564</td>\n",
       "      <td>-0.4432</td>\n",
       "      <td>-0.3778</td>\n",
       "      <td>-0.4530</td>\n",
       "      <td>6.9004</td>\n",
       "      <td>...</td>\n",
       "      <td>103.9529</td>\n",
       "      <td>64.9225</td>\n",
       "      <td>62.4355</td>\n",
       "      <td>95.2780</td>\n",
       "      <td>65.5114</td>\n",
       "      <td>64.4457</td>\n",
       "      <td>0.2895</td>\n",
       "      <td>0.2997</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>Arborio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>213.9765</td>\n",
       "      <td>212.5912</td>\n",
       "      <td>229.1017</td>\n",
       "      <td>23.9463</td>\n",
       "      <td>24.2106</td>\n",
       "      <td>25.4296</td>\n",
       "      <td>-0.8286</td>\n",
       "      <td>-0.7967</td>\n",
       "      <td>-1.1883</td>\n",
       "      <td>3.8488</td>\n",
       "      <td>...</td>\n",
       "      <td>108.9778</td>\n",
       "      <td>65.4571</td>\n",
       "      <td>59.9502</td>\n",
       "      <td>100.2301</td>\n",
       "      <td>67.5089</td>\n",
       "      <td>63.6028</td>\n",
       "      <td>0.3335</td>\n",
       "      <td>0.3426</td>\n",
       "      <td>0.4257</td>\n",
       "      <td>Karacadag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>204.7994</td>\n",
       "      <td>207.2316</td>\n",
       "      <td>216.9780</td>\n",
       "      <td>16.1357</td>\n",
       "      <td>16.4876</td>\n",
       "      <td>16.4977</td>\n",
       "      <td>-1.2066</td>\n",
       "      <td>-1.2433</td>\n",
       "      <td>-1.3866</td>\n",
       "      <td>5.9842</td>\n",
       "      <td>...</td>\n",
       "      <td>106.0881</td>\n",
       "      <td>64.1869</td>\n",
       "      <td>61.3876</td>\n",
       "      <td>97.1585</td>\n",
       "      <td>66.2445</td>\n",
       "      <td>63.0596</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.3164</td>\n",
       "      <td>0.3761</td>\n",
       "      <td>Arborio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>234.8254</td>\n",
       "      <td>236.9970</td>\n",
       "      <td>233.0475</td>\n",
       "      <td>9.9312</td>\n",
       "      <td>9.3609</td>\n",
       "      <td>9.0690</td>\n",
       "      <td>-2.4975</td>\n",
       "      <td>-3.4904</td>\n",
       "      <td>-3.2237</td>\n",
       "      <td>21.9007</td>\n",
       "      <td>...</td>\n",
       "      <td>119.2037</td>\n",
       "      <td>63.3545</td>\n",
       "      <td>64.8200</td>\n",
       "      <td>109.3027</td>\n",
       "      <td>63.3122</td>\n",
       "      <td>63.5967</td>\n",
       "      <td>0.3970</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>0.4469</td>\n",
       "      <td>Ipsala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>240.6182</td>\n",
       "      <td>241.7075</td>\n",
       "      <td>236.3982</td>\n",
       "      <td>8.6521</td>\n",
       "      <td>8.6222</td>\n",
       "      <td>8.5961</td>\n",
       "      <td>-4.9902</td>\n",
       "      <td>-5.3339</td>\n",
       "      <td>-4.2648</td>\n",
       "      <td>47.2017</td>\n",
       "      <td>...</td>\n",
       "      <td>121.4198</td>\n",
       "      <td>63.5424</td>\n",
       "      <td>65.2355</td>\n",
       "      <td>111.4580</td>\n",
       "      <td>63.0129</td>\n",
       "      <td>63.9117</td>\n",
       "      <td>0.4162</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.4626</td>\n",
       "      <td>Ipsala</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75000 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         meanRR    meanRG    meanRB  StdDevRR  StdDevRG  StdDevRB  skewRR  \\\n",
       "0      222.9805  223.9872  241.4758   16.2950   16.4354   13.6272 -0.7986   \n",
       "1      206.0380  206.2412  213.3809   18.3863   18.5343   18.9969 -0.7536   \n",
       "2      201.8228  217.6475  235.0057   13.7392   15.3239   16.0249 -2.2606   \n",
       "3      228.8978  229.7151  244.6294   18.3915   18.9141   16.7398 -1.5281   \n",
       "4      210.4471  210.2988  220.8827   19.6370   19.6635   20.0206 -1.0165   \n",
       "...         ...       ...       ...       ...       ...       ...     ...   \n",
       "74995  204.4210  201.7319  208.9247   13.8755   13.8819   14.0564 -0.4432   \n",
       "74996  213.9765  212.5912  229.1017   23.9463   24.2106   25.4296 -0.8286   \n",
       "74997  204.7994  207.2316  216.9780   16.1357   16.4876   16.4977 -1.2066   \n",
       "74998  234.8254  236.9970  233.0475    9.9312    9.3609    9.0690 -2.4975   \n",
       "74999  240.6182  241.7075  236.3982    8.6521    8.6222    8.5961 -4.9902   \n",
       "\n",
       "       skewRG  skewRB  kurtosisRR  ...  ALLdaub4L  ALLdaub4a  ALLdaub4b  \\\n",
       "0     -0.8407 -3.7377      7.1706  ...   113.9924    65.0610    59.5989   \n",
       "1     -0.7372 -0.8217      4.4926  ...   105.7055    64.3685    62.2084   \n",
       "2     -2.6764 -2.8690     12.4329  ...   109.7155    62.6423    58.7439   \n",
       "3     -1.4967 -3.5705      8.1541  ...   116.5405    64.9069    60.2562   \n",
       "4     -1.0176 -1.1514      4.6467  ...   107.7502    64.7071    61.3549   \n",
       "...       ...     ...         ...  ...        ...        ...        ...   \n",
       "74995 -0.3778 -0.4530      6.9004  ...   103.9529    64.9225    62.4355   \n",
       "74996 -0.7967 -1.1883      3.8488  ...   108.9778    65.4571    59.9502   \n",
       "74997 -1.2433 -1.3866      5.9842  ...   106.0881    64.1869    61.3876   \n",
       "74998 -3.4904 -3.2237     21.9007  ...   119.2037    63.3545    64.8200   \n",
       "74999 -5.3339 -4.2648     47.2017  ...   121.4198    63.5424    65.2355   \n",
       "\n",
       "       ALLdaub4Y  ALLdaub4Cb  ALLdaub4Cr  ALLdaub4XX  ALLdaub4YY  ALLdaub4ZZ  \\\n",
       "0       104.8552     67.8779     63.0828      0.3673      0.3793      0.4733   \n",
       "1        96.8375     65.5371     63.5832      0.3014      0.3144      0.3641   \n",
       "2       100.2352     68.9753     59.8342      0.3233      0.3445      0.4448   \n",
       "3       107.2560     67.3298     63.2237      0.3880      0.4020      0.4904   \n",
       "4        98.8704     66.2048     63.5378      0.3184      0.3303      0.3928   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "74995    95.2780     65.5114     64.4457      0.2895      0.2997      0.3455   \n",
       "74996   100.2301     67.5089     63.6028      0.3335      0.3426      0.4257   \n",
       "74997    97.1585     66.2445     63.0596      0.3028      0.3164      0.3761   \n",
       "74998   109.3027     63.3122     63.5967      0.3970      0.4215      0.4469   \n",
       "74999   111.4580     63.0129     63.9117      0.4162      0.4414      0.4626   \n",
       "\n",
       "           CLASS  \n",
       "0        Basmati  \n",
       "1        Arborio  \n",
       "2        Jasmine  \n",
       "3        Basmati  \n",
       "4        Arborio  \n",
       "...          ...  \n",
       "74995    Arborio  \n",
       "74996  Karacadag  \n",
       "74997    Arborio  \n",
       "74998     Ipsala  \n",
       "74999     Ipsala  \n",
       "\n",
       "[75000 rows x 91 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05f14884-4361-4576-b260-fa86e4b5a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.array(df)\n",
    "df = pd.DataFrame(df)\n",
    "X = df.drop(columns = [90])\n",
    "y = df[90]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a576792-2434-4582-bbcb-3a9aca0264b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 :\n",
      "MLP: Acc=0.9920, Err=0.0080, Prec=0.9920, Rec=0.9920, F1=0.9920, Kappa=0.9900\n",
      "Random Forest: Acc=0.9873, Err=0.0127, Prec=0.9873, Rec=0.9873, F1=0.9873, Kappa=0.9842\n",
      "SVM: Acc=0.9919, Err=0.0081, Prec=0.9919, Rec=0.9919, F1=0.9919, Kappa=0.9898\n",
      "KNN: Acc=0.9844, Err=0.0156, Prec=0.9844, Rec=0.9844, F1=0.9844, Kappa=0.9805\n",
      "Logistic Regression: Acc=0.9909, Err=0.0091, Prec=0.9909, Rec=0.9909, F1=0.9909, Kappa=0.9887\n",
      "Decision Tree: Acc=0.9740, Err=0.0260, Prec=0.9740, Rec=0.9740, F1=0.9740, Kappa=0.9675\n",
      "--------------------------------------------------\n",
      "Fold 2 :\n",
      "MLP: Acc=0.9939, Err=0.0061, Prec=0.9939, Rec=0.9939, F1=0.9939, Kappa=0.9923\n",
      "Random Forest: Acc=0.9904, Err=0.0096, Prec=0.9904, Rec=0.9904, F1=0.9904, Kappa=0.9880\n",
      "SVM: Acc=0.9924, Err=0.0076, Prec=0.9924, Rec=0.9924, F1=0.9924, Kappa=0.9905\n",
      "KNN: Acc=0.9857, Err=0.0143, Prec=0.9858, Rec=0.9857, F1=0.9857, Kappa=0.9822\n",
      "Logistic Regression: Acc=0.9905, Err=0.0095, Prec=0.9905, Rec=0.9905, F1=0.9905, Kappa=0.9882\n",
      "Decision Tree: Acc=0.9780, Err=0.0220, Prec=0.9780, Rec=0.9780, F1=0.9780, Kappa=0.9725\n",
      "--------------------------------------------------\n",
      "Fold 3 :\n",
      "MLP: Acc=0.9916, Err=0.0084, Prec=0.9916, Rec=0.9916, F1=0.9916, Kappa=0.9895\n",
      "Random Forest: Acc=0.9860, Err=0.0140, Prec=0.9860, Rec=0.9860, F1=0.9860, Kappa=0.9825\n",
      "SVM: Acc=0.9920, Err=0.0080, Prec=0.9920, Rec=0.9920, F1=0.9920, Kappa=0.9900\n",
      "KNN: Acc=0.9825, Err=0.0175, Prec=0.9825, Rec=0.9825, F1=0.9825, Kappa=0.9782\n",
      "Logistic Regression: Acc=0.9888, Err=0.0112, Prec=0.9888, Rec=0.9888, F1=0.9888, Kappa=0.9860\n",
      "Decision Tree: Acc=0.9711, Err=0.0289, Prec=0.9711, Rec=0.9711, F1=0.9711, Kappa=0.9638\n",
      "--------------------------------------------------\n",
      "Fold 4 :\n",
      "MLP: Acc=0.9895, Err=0.0105, Prec=0.9895, Rec=0.9895, F1=0.9895, Kappa=0.9868\n",
      "Random Forest: Acc=0.9853, Err=0.0147, Prec=0.9854, Rec=0.9853, F1=0.9853, Kappa=0.9817\n",
      "SVM: Acc=0.9907, Err=0.0093, Prec=0.9907, Rec=0.9907, F1=0.9907, Kappa=0.9883\n",
      "KNN: Acc=0.9824, Err=0.0176, Prec=0.9824, Rec=0.9824, F1=0.9824, Kappa=0.9780\n",
      "Logistic Regression: Acc=0.9891, Err=0.0109, Prec=0.9891, Rec=0.9891, F1=0.9891, Kappa=0.9863\n",
      "Decision Tree: Acc=0.9740, Err=0.0260, Prec=0.9740, Rec=0.9740, F1=0.9740, Kappa=0.9675\n",
      "--------------------------------------------------\n",
      "Fold 5 :\n",
      "MLP: Acc=0.9913, Err=0.0087, Prec=0.9913, Rec=0.9913, F1=0.9913, Kappa=0.9892\n",
      "Random Forest: Acc=0.9883, Err=0.0117, Prec=0.9883, Rec=0.9883, F1=0.9883, Kappa=0.9853\n",
      "SVM: Acc=0.9921, Err=0.0079, Prec=0.9921, Rec=0.9921, F1=0.9921, Kappa=0.9902\n",
      "KNN: Acc=0.9848, Err=0.0152, Prec=0.9848, Rec=0.9848, F1=0.9848, Kappa=0.9810\n",
      "Logistic Regression: Acc=0.9892, Err=0.0108, Prec=0.9892, Rec=0.9892, F1=0.9892, Kappa=0.9865\n",
      "Decision Tree: Acc=0.9717, Err=0.0283, Prec=0.9717, Rec=0.9717, F1=0.9717, Kappa=0.9647\n",
      "--------------------------------------------------\n",
      "Fold 6 :\n",
      "MLP: Acc=0.9919, Err=0.0081, Prec=0.9919, Rec=0.9919, F1=0.9919, Kappa=0.9898\n",
      "Random Forest: Acc=0.9852, Err=0.0148, Prec=0.9852, Rec=0.9852, F1=0.9852, Kappa=0.9815\n",
      "SVM: Acc=0.9913, Err=0.0087, Prec=0.9913, Rec=0.9913, F1=0.9913, Kappa=0.9892\n",
      "KNN: Acc=0.9832, Err=0.0168, Prec=0.9832, Rec=0.9832, F1=0.9832, Kappa=0.9790\n",
      "Logistic Regression: Acc=0.9889, Err=0.0111, Prec=0.9889, Rec=0.9889, F1=0.9889, Kappa=0.9862\n",
      "Decision Tree: Acc=0.9712, Err=0.0288, Prec=0.9712, Rec=0.9712, F1=0.9712, Kappa=0.9640\n",
      "--------------------------------------------------\n",
      "Fold 7 :\n",
      "MLP: Acc=0.9909, Err=0.0091, Prec=0.9909, Rec=0.9909, F1=0.9909, Kappa=0.9887\n",
      "Random Forest: Acc=0.9845, Err=0.0155, Prec=0.9846, Rec=0.9845, F1=0.9845, Kappa=0.9807\n",
      "SVM: Acc=0.9907, Err=0.0093, Prec=0.9907, Rec=0.9907, F1=0.9907, Kappa=0.9883\n",
      "KNN: Acc=0.9820, Err=0.0180, Prec=0.9820, Rec=0.9820, F1=0.9820, Kappa=0.9775\n",
      "Logistic Regression: Acc=0.9885, Err=0.0115, Prec=0.9885, Rec=0.9885, F1=0.9885, Kappa=0.9857\n",
      "Decision Tree: Acc=0.9725, Err=0.0275, Prec=0.9726, Rec=0.9725, F1=0.9725, Kappa=0.9657\n",
      "--------------------------------------------------\n",
      "Fold 8 :\n",
      "MLP: Acc=0.9943, Err=0.0057, Prec=0.9943, Rec=0.9943, F1=0.9943, Kappa=0.9928\n",
      "Random Forest: Acc=0.9856, Err=0.0144, Prec=0.9856, Rec=0.9856, F1=0.9856, Kappa=0.9820\n",
      "SVM: Acc=0.9909, Err=0.0091, Prec=0.9909, Rec=0.9909, F1=0.9909, Kappa=0.9887\n",
      "KNN: Acc=0.9844, Err=0.0156, Prec=0.9844, Rec=0.9844, F1=0.9844, Kappa=0.9805\n",
      "Logistic Regression: Acc=0.9883, Err=0.0117, Prec=0.9883, Rec=0.9883, F1=0.9883, Kappa=0.9853\n",
      "Decision Tree: Acc=0.9727, Err=0.0273, Prec=0.9727, Rec=0.9727, F1=0.9727, Kappa=0.9658\n",
      "--------------------------------------------------\n",
      "Fold 9 :\n",
      "MLP: Acc=0.9935, Err=0.0065, Prec=0.9935, Rec=0.9935, F1=0.9935, Kappa=0.9918\n",
      "Random Forest: Acc=0.9875, Err=0.0125, Prec=0.9875, Rec=0.9875, F1=0.9875, Kappa=0.9843\n",
      "SVM: Acc=0.9904, Err=0.0096, Prec=0.9904, Rec=0.9904, F1=0.9904, Kappa=0.9880\n",
      "KNN: Acc=0.9827, Err=0.0173, Prec=0.9827, Rec=0.9827, F1=0.9827, Kappa=0.9783\n",
      "Logistic Regression: Acc=0.9889, Err=0.0111, Prec=0.9889, Rec=0.9889, F1=0.9889, Kappa=0.9862\n",
      "Decision Tree: Acc=0.9751, Err=0.0249, Prec=0.9751, Rec=0.9751, F1=0.9751, Kappa=0.9688\n",
      "--------------------------------------------------\n",
      "Fold 10 :\n",
      "MLP: Acc=0.9916, Err=0.0084, Prec=0.9917, Rec=0.9916, F1=0.9916, Kappa=0.9895\n",
      "Random Forest: Acc=0.9833, Err=0.0167, Prec=0.9834, Rec=0.9833, F1=0.9833, Kappa=0.9792\n",
      "SVM: Acc=0.9897, Err=0.0103, Prec=0.9898, Rec=0.9897, F1=0.9897, Kappa=0.9872\n",
      "KNN: Acc=0.9812, Err=0.0188, Prec=0.9812, Rec=0.9812, F1=0.9812, Kappa=0.9765\n",
      "Logistic Regression: Acc=0.9875, Err=0.0125, Prec=0.9875, Rec=0.9875, F1=0.9875, Kappa=0.9843\n",
      "Decision Tree: Acc=0.9715, Err=0.0285, Prec=0.9716, Rec=0.9715, F1=0.9715, Kappa=0.9643\n",
      "--------------------------------------------------\n",
      "+---------------------+-----------+-------------+------------+--------------+----------+-------------+\n",
      "| Model               |   Avg Acc |   Avg Error |   Avg Prec |   Avg Recall |   Avg F1 |   Avg Kappa |\n",
      "+=====================+===========+=============+============+==============+==========+=============+\n",
      "| MLP                 |    0.992  |      0.008  |     0.9921 |       0.992  |   0.992  |      0.99   |\n",
      "+---------------------+-----------+-------------+------------+--------------+----------+-------------+\n",
      "| Random Forest       |    0.9863 |      0.0137 |     0.9864 |       0.9863 |   0.9863 |      0.9829 |\n",
      "+---------------------+-----------+-------------+------------+--------------+----------+-------------+\n",
      "| SVM                 |    0.9912 |      0.0088 |     0.9912 |       0.9912 |   0.9912 |      0.989  |\n",
      "+---------------------+-----------+-------------+------------+--------------+----------+-------------+\n",
      "| KNN                 |    0.9833 |      0.0167 |     0.9834 |       0.9833 |   0.9833 |      0.9792 |\n",
      "+---------------------+-----------+-------------+------------+--------------+----------+-------------+\n",
      "| Logistic Regression |    0.9891 |      0.0109 |     0.9891 |       0.9891 |   0.9891 |      0.9863 |\n",
      "+---------------------+-----------+-------------+------------+--------------+----------+-------------+\n",
      "| Decision Tree       |    0.9732 |      0.0268 |     0.9732 |       0.9732 |   0.9732 |      0.9665 |\n",
      "+---------------------+-----------+-------------+------------+--------------+----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "models = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(64, 32, 16, 8), activation='logistic', max_iter=10000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel='linear', random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=10),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "model_metrics = {name: {\"Accuracy\": [], \"Error\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Kappa\": []} for name in models.keys()}\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(df)):\n",
    "    print(f\"Fold {fold + 1} :\")\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_idx, :], X.iloc[test_idx, :]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        error = 1 - acc  # Error rate\n",
    "        prec = precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "        model_metrics[name][\"Accuracy\"].append(acc)\n",
    "        model_metrics[name][\"Error\"].append(error)\n",
    "        model_metrics[name][\"Precision\"].append(prec)\n",
    "        model_metrics[name][\"Recall\"].append(rec)\n",
    "        model_metrics[name][\"F1-Score\"].append(f1)\n",
    "        model_metrics[name][\"Kappa\"].append(kappa)\n",
    "\n",
    "        print(f\"{name}: Acc={acc:.4f}, Err={error:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}, Kappa={kappa:.4f}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "table_data = [\n",
    "    [name, \n",
    "     f\"{np.mean(metrics['Accuracy']):.4f}\", \n",
    "     f\"{np.mean(metrics['Error']):.4f}\", \n",
    "     f\"{np.mean(metrics['Precision']):.4f}\", \n",
    "     f\"{np.mean(metrics['Recall']):.4f}\", \n",
    "     f\"{np.mean(metrics['F1-Score']):.4f}\", \n",
    "     f\"{np.mean(metrics['Kappa']):.4f}\"]\n",
    "    for name, metrics in model_metrics.items()\n",
    "]\n",
    "\n",
    "print(tabulate(table_data, headers=[\"Model\", \"Avg Acc\", \"Avg Error\", \"Avg Prec\", \"Avg Recall\", \"Avg F1\", \"Avg Kappa\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0833dc38-5c6f-43fd-acdd-b809a4c6fb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 :\n",
      "MLP\n",
      "Random Forest\n",
      "SVM\n",
      "KNN\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Fold 1 Completed\n",
      "--------------------------------------------------\n",
      "Fold 2 :\n",
      "MLP\n",
      "Random Forest\n",
      "SVM\n",
      "KNN\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Fold 2 Completed\n",
      "--------------------------------------------------\n",
      "Fold 3 :\n",
      "MLP\n",
      "Random Forest\n",
      "SVM\n",
      "KNN\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Fold 3 Completed\n",
      "--------------------------------------------------\n",
      "Fold 4 :\n",
      "MLP\n",
      "Random Forest\n",
      "SVM\n",
      "KNN\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Fold 4 Completed\n",
      "--------------------------------------------------\n",
      "Fold 5 :\n",
      "MLP\n",
      "Random Forest\n",
      "SVM\n",
      "KNN\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Fold 5 Completed\n",
      "--------------------------------------------------\n",
      "Fold 6 :\n",
      "MLP\n",
      "Random Forest\n",
      "SVM\n",
      "KNN\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Fold 6 Completed\n",
      "--------------------------------------------------\n",
      "Fold 7 :\n",
      "MLP\n",
      "Random Forest\n",
      "SVM\n",
      "KNN\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Fold 7 Completed\n",
      "--------------------------------------------------\n",
      "Fold 8 :\n",
      "MLP\n",
      "Random Forest\n",
      "SVM\n",
      "KNN\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Fold 8 Completed\n",
      "--------------------------------------------------\n",
      "Fold 9 :\n",
      "MLP\n",
      "Random Forest\n",
      "SVM\n",
      "KNN\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Fold 9 Completed\n",
      "--------------------------------------------------\n",
      "Fold 10 :\n",
      "MLP\n",
      "Random Forest\n",
      "SVM\n",
      "KNN\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Fold 10 Completed\n",
      "--------------------------------------------------\n",
      "   Actual Class            Algorithm  Arborio  Basmati  Ipsala  Jasmine  \\\n",
      "0       Arborio                  MLP    14829        9       0      143   \n",
      "1       Arborio        Random Forest    14780        5       0      198   \n",
      "2       Arborio                  SVM    14811       10       0      172   \n",
      "3       Arborio                  KNN    14750        3       1      237   \n",
      "4       Arborio  Logistic Regression    14791       16       0      178   \n",
      "5       Arborio        Decision Tree    14613       47       6      265   \n",
      "6       Basmati                  MLP       13    14898       1       12   \n",
      "7       Basmati        Random Forest       38    14647       1       15   \n",
      "8       Basmati                  SVM       21    14844       0       22   \n",
      "9       Basmati                  KNN       61    14665       1       37   \n",
      "10      Basmati  Logistic Regression       22    14778       2       65   \n",
      "11      Basmati        Decision Tree       46    14389       5       46   \n",
      "12       Ipsala                  MLP        5        2   14993        0   \n",
      "13       Ipsala        Random Forest        0        0   15000        0   \n",
      "14       Ipsala                  SVM        0        0   15000        0   \n",
      "15       Ipsala                  KNN        1        0   14999        0   \n",
      "16       Ipsala  Logistic Regression        0        0   15000        0   \n",
      "17       Ipsala        Decision Tree        4        5   14987        3   \n",
      "18      Jasmine                  MLP      145       10       0    14823   \n",
      "19      Jasmine        Random Forest      115       14       0    14848   \n",
      "20      Jasmine                  SVM      104       18       0    14862   \n",
      "21      Jasmine                  KNN      175       17       0    14781   \n",
      "22      Jasmine  Logistic Regression      113       42       0    14813   \n",
      "23      Jasmine        Decision Tree      259       51       2    14626   \n",
      "24    Karacadag                  MLP       19      108       0       13   \n",
      "25    Karacadag        Random Forest       50      218       0       31   \n",
      "26    Karacadag                  SVM       23      129       0       24   \n",
      "27    Karacadag                  KNN       68      292       0       85   \n",
      "28    Karacadag  Logistic Regression       28      145       0       29   \n",
      "29    Karacadag        Decision Tree       64      483       1       79   \n",
      "\n",
      "    Karacadag  \n",
      "0          19  \n",
      "1          17  \n",
      "2           7  \n",
      "3           9  \n",
      "4          15  \n",
      "5          69  \n",
      "6          76  \n",
      "7         299  \n",
      "8         113  \n",
      "9         236  \n",
      "10        133  \n",
      "11        514  \n",
      "12          0  \n",
      "13          0  \n",
      "14          0  \n",
      "15          0  \n",
      "16          0  \n",
      "17          1  \n",
      "18         22  \n",
      "19         23  \n",
      "20         16  \n",
      "21         27  \n",
      "22         32  \n",
      "23         62  \n",
      "24      14860  \n",
      "25      14701  \n",
      "26      14824  \n",
      "27      14555  \n",
      "28      14798  \n",
      "29      14373  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = np.unique(y)\n",
    "conf_matrix_sum = {name: np.zeros((len(labels), len(labels))) for name in models.keys()}\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(df)):\n",
    "    print(f\"Fold {fold + 1} :\")\n",
    "    X_train, X_test = X.iloc[train_idx, :], X.iloc[test_idx, :]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "        conf_matrix_sum[name] += cm\n",
    "        print(f\"{name}\")\n",
    "\n",
    "    print(f\"Fold {fold + 1} Completed\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "conf_matrix_total = {name: cm.astype(int) for name, cm in conf_matrix_sum.items()}\n",
    "\n",
    "conf_matrix_data = []\n",
    "for i, actual in enumerate(labels):\n",
    "    for name, cm in conf_matrix_total.items():\n",
    "        row = [actual, name]\n",
    "        row.extend(cm[i])\n",
    "        conf_matrix_data.append(row)\n",
    "\n",
    "columns = [\"Actual Class\", \"Algorithm\"] + list(labels)\n",
    "df_conf_matrix = pd.DataFrame(conf_matrix_data, columns=columns)\n",
    "\n",
    "print(df_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71b08c51-5d93-4933-be6e-a92163970d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "|    | Actual Class   | Algorithm           |   Arborio |   Basmati |   Ipsala |   Jasmine |   Karacadag |\n",
      "+====+================+=====================+===========+===========+==========+===========+=============+\n",
      "|  0 | Arborio        | MLP                 |     14829 |         9 |        0 |       143 |          19 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "|  1 | Arborio        | Random Forest       |     14780 |         5 |        0 |       198 |          17 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "|  2 | Arborio        | SVM                 |     14811 |        10 |        0 |       172 |           7 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "|  3 | Arborio        | KNN                 |     14750 |         3 |        1 |       237 |           9 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "|  4 | Arborio        | Logistic Regression |     14791 |        16 |        0 |       178 |          15 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "|  5 | Arborio        | Decision Tree       |     14613 |        47 |        6 |       265 |          69 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "|  6 | Basmati        | MLP                 |        13 |     14898 |        1 |        12 |          76 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "|  7 | Basmati        | Random Forest       |        38 |     14647 |        1 |        15 |         299 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "|  8 | Basmati        | SVM                 |        21 |     14844 |        0 |        22 |         113 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "|  9 | Basmati        | KNN                 |        61 |     14665 |        1 |        37 |         236 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 10 | Basmati        | Logistic Regression |        22 |     14778 |        2 |        65 |         133 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 11 | Basmati        | Decision Tree       |        46 |     14389 |        5 |        46 |         514 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 12 | Ipsala         | MLP                 |         5 |         2 |    14993 |         0 |           0 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 13 | Ipsala         | Random Forest       |         0 |         0 |    15000 |         0 |           0 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 14 | Ipsala         | SVM                 |         0 |         0 |    15000 |         0 |           0 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 15 | Ipsala         | KNN                 |         1 |         0 |    14999 |         0 |           0 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 16 | Ipsala         | Logistic Regression |         0 |         0 |    15000 |         0 |           0 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 17 | Ipsala         | Decision Tree       |         4 |         5 |    14987 |         3 |           1 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 18 | Jasmine        | MLP                 |       145 |        10 |        0 |     14823 |          22 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 19 | Jasmine        | Random Forest       |       115 |        14 |        0 |     14848 |          23 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 20 | Jasmine        | SVM                 |       104 |        18 |        0 |     14862 |          16 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 21 | Jasmine        | KNN                 |       175 |        17 |        0 |     14781 |          27 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 22 | Jasmine        | Logistic Regression |       113 |        42 |        0 |     14813 |          32 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 23 | Jasmine        | Decision Tree       |       259 |        51 |        2 |     14626 |          62 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 24 | Karacadag      | MLP                 |        19 |       108 |        0 |        13 |       14860 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 25 | Karacadag      | Random Forest       |        50 |       218 |        0 |        31 |       14701 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 26 | Karacadag      | SVM                 |        23 |       129 |        0 |        24 |       14824 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 27 | Karacadag      | KNN                 |        68 |       292 |        0 |        85 |       14555 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 28 | Karacadag      | Logistic Regression |        28 |       145 |        0 |        29 |       14798 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n",
      "| 29 | Karacadag      | Decision Tree       |        64 |       483 |        1 |        79 |       14373 |\n",
      "+----+----------------+---------------------+-----------+-----------+----------+-----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(df_conf_matrix, headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1170f71-f828-4d5d-ba5b-5cc4537344eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
